import streamlit as st

st.title("keiba AI")

## import & def func
import re
# import os                                   # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ“ä½œ
# import csv
import time
# import json
import datetime
# import random
import requests

# from IPython.core.display import display
from bs4 import BeautifulSoup
from tqdm import tqdm

from itertools import *
from collections import *

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# import seaborn as sns

# import japanize_matplotlib

# æ©Ÿæ¢°å­¦ç¿’
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, QuantileTransformer, StandardScaler, OrdinalEncoder
from sklearn.model_selection import train_test_split, GridSearchCV, validation_curve, KFold, StratifiedKFold
from sklearn.metrics import roc_auc_score, mean_absolute_error, mean_squared_error
from sklearn import preprocessing
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.linear_model import LogisticRegression
# from sklearn.neural_network import MLPClassifier
# from sklearn.neighbors import  KNeighborsClassifier, KNeighborsRegressor
# from sklearn import svm
import lightgbm as lgbm
# import xgboost as xgb


import warnings
warnings.filterwarnings('ignore')

import pickle
import dill as pickle

from scipy.misc import derivative

# import os 
# print(os.getcwd())
# print(os.listdir())

class Name:
    def __init__(self):
        self.name_dict = {}
        self.notation = {}
        self.race_name = {}
    def __getitem__(self, id):
        return self.name_dict[id]
    def __setitem__(self, id, name):
        path_list = re.findall("\w+", id)
        if len(path_list)==2 or (len(path_list)==3 and path_list[1]=="ped"):
            pass
        else:
            # print(id)
            pass

        if id in self.name_dict:
            self.notation[id].add(name)
        else:
            self.name_dict[id] = name 
            self.notation[id] = set([name])
    def get_multi_notations(self):
        return {id: value for id, value in nm.notation.items() if len(value)>=2}

def preprocess_result(_df):
    # ã„ã‚‰ãªã„ columns æ¶ˆå»
    _df = _df.drop(columns=["ï¾€ï½²ï¾‘æŒ‡æ•°", "èª¿æ•™ï¾€ï½²ï¾‘", "å©èˆï½ºï¾’ï¾ï¾„", "å‚™è€ƒ"])
    _df = _df.drop(columns=["ç€å·®"])

    _df = _df[~_df["ç€é †"].isin(["ä¸­","é™¤","å–","å¤±"])]
    _df["ç€é †"] = _df["ç€é †"].astype(str).map(lambda x: x[:x.index("(")] if "(" in x else x).astype(float)
    assert set(_df["ç€é †"].fillna(1)) <= set(range(1,19))

    _df["æ ç•ª"] = _df["æ ç•ª"].astype(float)
    assert set(_df["æ ç•ª"].fillna(1)) <= set(range(1,9))

    _df["é¦¬ç•ª"] = _df["é¦¬ç•ª"].astype(float)
    assert set(_df["é¦¬ç•ª"].fillna(1)) <= set(range(1,19))

    _df["æ€§"] = _df["æ€§é½¢"].str[0].astype("category")
    assert set(_df["æ€§"]) <= set(["ç‰¡", "ç‰", "ã‚»"])
    _df["é½¢"] = _df["æ€§é½¢"].str[1:].astype(float)
    assert set(_df["é½¢"]) <= set(range(2,20))
    _df = _df.drop(columns=["æ€§é½¢"])

    _df["æ–¤é‡"] = _df["æ–¤é‡"].astype(float)
    assert all(45<=x<=65 for x in _df["æ–¤é‡"])

    def f(x):
        if str(x) == "nan":
            return np.nan
        return int(x[0])*60 + float(x[2:])
    _df["ã‚¿ã‚¤ãƒ "] = _df["ã‚¿ã‚¤ãƒ "].map(f)
    assert all(50<=x<=400 for x in _df["ã‚¿ã‚¤ãƒ "].fillna(100))

    _df["å˜å‹"] = _df["å˜å‹"].astype(float)
    assert all(1<=x<=10000 for x in _df["å˜å‹"].fillna(1))

    _df["äººæ°—"] = _df["äººæ°—"].astype(float)
    assert set(_df["äººæ°—"].fillna(1)) <= set(range(1,19))

    def f(x):
        if x in {"", "nan"}:
            return "nan(nan)"
        x = x.replace("å‰è¨ˆä¸", "nan").replace("è¨ˆä¸", "nan")
        return x
    _df["é¦¬ä½“é‡"] = _df["é¦¬ä½“é‡"].astype(str).map(f)

    _df["ä½“é‡"] = _df["é¦¬ä½“é‡"].map(lambda x: x[:3]).astype(float)
    assert all(300<=x<=700 for x in _df["ä½“é‡"].fillna(400))
    _df["å¢—æ¸›"] = _df["é¦¬ä½“é‡"].map(lambda x: x[4:-1]).astype(float)
    assert all(-100<=x<=100 for x in _df["å¢—æ¸›"].fillna(0))
    _df = _df.drop(columns=["é¦¬ä½“é‡"])

    _df["æ±è¥¿"] = _df["èª¿æ•™å¸«"].map(lambda x: x[1:2]).astype("category")
    assert set(_df["æ±è¥¿"]) <= set("æ±è¥¿åœ°å¤–")

    _df["èª¿æ•™å¸«"] = _df["èª¿æ•™å¸«"].map(lambda x: x[3:])

    _df["è³é‡‘(ä¸‡å††)"] = _df["è³é‡‘(ä¸‡å††)"].fillna("0").map(lambda x: x.replace(",","")).astype(float)
    _df = _df.rename(columns={'è³é‡‘(ä¸‡å††)': 'è³é‡‘'})
    assert all(0<=x<=40000 for x in set(_df["è³é‡‘"]))

    _df["é€šé"] = _df["é€šé"].fillna("nan").map(lambda x: np.array(x.split("-"), dtype=float) )
    # assert all( type(x) is type(np.array([])) for x in set(_df["é€šé"]) )


    # ID
    for col_name, col_id in (("é¦¬å", "horse_id"), ("é¨æ‰‹", "jockey_id"), ("èª¿æ•™å¸«", "trainer_id"), ("é¦¬ä¸»", "owner_id")):
        _df[col_id] = _df[col_id].astype(str).map(lambda x: x.replace("/result/recent", "")).replace({"nan": np.nan})
        for name, id in _df[[col_name, col_id]].values:
            if id == id:
                nm[id] = name
        _df = _df.drop(columns=[col_name])
        _df[col_id] = _df[col_id].astype("category")



    # for col, num_null in _df.isnull().sum().items():
    #     if num_null:
    #         print(col, num_null)
    #         display(_df[_df[col].isnull()])



    return _df

def preprocess_race(_df):
    _df = _df.drop(columns=["éå»ãƒ¬ãƒ¼ã‚¹url", "ãƒ¬ãƒ¼ã‚¹ç·ç§°"])

    _df["èŠãƒ€"] = _df["ã‚³ãƒ¼ã‚¹"].map(lambda x: x[0]).astype("category")
    _df = _df[~_df["èŠãƒ€"].isin(["éšœ"])]
    assert set(_df["èŠãƒ€"]) <= set(["ãƒ€", "èŠ"])

    _df["å›ã‚Š"] = _df["ã‚³ãƒ¼ã‚¹"].map(lambda x: x[1:-5]).astype("category")
    assert set(_df["å›ã‚Š"]) <= {'å³', 'å³å†…2å‘¨', 'å³å¤–', 'å³å¤–-å†…', 'å·¦', 'å·¦å¤–', 'ç›´ç·š'}

    _df["è·é›¢"] = _df["ã‚³ãƒ¼ã‚¹"].map(lambda x: x[-5:-1]).astype(float)
    assert all(1000<=x<=4000 for x in set(_df["è·é›¢"]))

    _df = _df.drop(columns=["ã‚³ãƒ¼ã‚¹"])

    _df["å¤©æ°—"] = _df["å¤©æ°—"].astype("category")
    assert all(x in {'å°é›¨', 'å°é›ª', 'æ™´', 'æ›‡', 'é›¨', 'é›ª'} or np.isnan(x) for x in _df["å¤©æ°—"])

    _df["é¦¬å ´"] = _df["é¦¬å ´"].astype("category")
    assert all(x in {'ä¸è‰¯', 'ç¨é‡', 'è‰¯', 'é‡'} or np.isnan(x) for x in _df["é¦¬å ´"])

    _df["æ™‚é–“"] = _df["æ™‚é–“"].map(lambda x: x[0:2]).astype(float) *60 + _df["æ™‚é–“"].map(lambda x: x[3:5]).astype(float)
    assert all(0<=x<=24*60 for x in set(_df["æ™‚é–“"].value_counts().index))

    _df["æ—¥ä»˜"] = _df["æ—¥ä»˜"].fillna("2002å¹´1æœˆ2æ—¥").map(lambda x: datetime.datetime.strptime(x, "%Yå¹´%mæœˆ%dæ—¥"))

    _df["å›"] = _df["é–‹å‚¬"].map(lambda x: x[0]).astype(float)
    assert set(_df["å›"]) <= set(range(1,7))
    _df["é–‹å‚¬åœ°"] = _df["é–‹å‚¬"].map(lambda x: x[2:4]).astype("category")
    assert set(_df["é–‹å‚¬åœ°"]) <= {'ä¸­äº¬', 'ä¸­å±±', 'äº¬éƒ½', 'å‡½é¤¨', 'å°å€‰', 'æ–°æ½Ÿ', 'æœ­å¹Œ', 'æ±äº¬', 'ç¦å³¶', 'é˜ªç¥'}
    _df["æ—¥ç›®"] = _df["é–‹å‚¬"].map(lambda x: int(x[4:-2] ))
    assert set(_df["æ—¥ç›®"]) <= set(range(1,13))
    _df = _df.drop(columns=["é–‹å‚¬"])

    def f(x):
        if x[2] == "ä»¥":
            return f"{x[0]}+"
            i = 4
        return x[0]
    _df["å¹´é½¢åˆ¶é™"] = _df["æ¡ä»¶"].map(f).astype("category")

    def f(x):
        if "(" in x:
            cl=x[x.index("(")+1:-1]
            if cl not in "LG":
                cl=cl[-2:]
            if cl=="G":
                cl="G3"
            return cl
        return ""
    _df["ã‚¯ãƒ©ã‚¹"] = _df["ãƒ¬ãƒ¼ã‚¹å"].map(f)
    def f(x):
        if "ä»¥ä¸Š" in x:
            return x[4:6]
        return x[2:4]
    _df["ã‚¯ãƒ©ã‚¹"] = np.where(
        _df["ã‚¯ãƒ©ã‚¹"]=="", 
        _df["æ¡ä»¶"].map(f).replace({"ã‚ªãƒ¼":"OP", "10": "2å‹", "16": "3å‹", "50": "1å‹"}), 
        _df["ã‚¯ãƒ©ã‚¹"]
        )
    _df["ã‚¯ãƒ©ã‚¹"] = _df["ã‚¯ãƒ©ã‚¹"].astype("category")
    _df = _df.drop(columns=["æ¡ä»¶"])


    for col_name, col_id in (("ãƒ¬ãƒ¼ã‚¹å", "race_id"), ):
        _df[col_id] = _df[col_id].map(lambda x: x.replace("/result/recent", ""))
        for name, id in _df[[col_name, col_id]].values:
            nm[id] = name
        _df[col_id] = _df[col_id].astype("category")
        _df = _df.drop(columns=[col_name])


    _df["ã‚¿ã‚¤ãƒ—"] = _df["ã‚¿ã‚¤ãƒ—"].astype(str).map(lambda x: x.replace("ãƒ»","").replace("ç‰¡ç‰",""))
    for col in ['æ··', 'ç‰¹æŒ‡', 'å®šé‡', 'å›½éš›', 'åˆ¥å®š', 'ãƒãƒ³ãƒ‡', 'æŒ‡', 'é¦¬é½¢', 'ç‰', 'è¦‹ç¿’é¨æ‰‹', 'ä¹å·ç”£é¦¬']:
        _df[col] = np.where( _df["ã‚¿ã‚¤ãƒ—"].map(lambda x: col in x), 1, 0)
    _df = _df.drop(columns=["ã‚¿ã‚¤ãƒ—"])



    _df["ãƒ©ãƒƒãƒ—"] = _df["ãƒ©ãƒƒãƒ—"].astype(str).map(lambda x: x.split(" - "))
    _df["race_ãƒ†ãƒ³"] = _df["ãƒšãƒ¼ã‚¹"].fillna("(nan-nan)").map(lambda x: x[x.index("(")+1:x.index(")")].split("-")[0]).astype(float)
    _df["race_ä¸Šã‚Š"] = _df["ãƒšãƒ¼ã‚¹"].fillna("(nan - nan)").map(lambda x: x[x.index("(")+1:x.index(")")].split("-")[1]).astype(float)
    # _df["ãƒšãƒ¼ã‚¹"] = _df["ãƒšãƒ¼ã‚¹"].map(lambda x: x[:x.index("(")].split(" - "))
    _df = _df.drop(columns=[f"{i}ã‚³ãƒ¼ãƒŠãƒ¼" for i in range(1,5)]+["ãƒ©ãƒƒãƒ—"]+["ãƒšãƒ¼ã‚¹"])

    return _df

def preprocess_pay(_df):
    return _df

def preprocess_prof(_df):

    _df = _df.drop(columns=["å‹Ÿé›†æƒ…å ±"], errors="ignore")

    _df["ç”Ÿå¹´æœˆæ—¥"] = _df["ç”Ÿå¹´æœˆæ—¥"].map(lambda x: datetime.datetime.strptime(x, "%Yå¹´%mæœˆ%dæ—¥"))
    _df["ç”£åœ°"] = _df["ç”£åœ°"].astype("category")
    _df["èª¿æ•™å¸«"]  = _df["èª¿æ•™å¸«"].fillna("()")
    _df["èª¿æ•™å ´æ‰€"] = _df["èª¿æ•™å¸«"].map(lambda x: x[x.index("(")+1:x.index(")")]).astype("category")
    _df["èª¿æ•™å¸«"] = _df["èª¿æ•™å¸«"].map(lambda x: x[:x.index("(")]).astype("category")

    _df["ã‚»ãƒªå–å¼•ä¾¡æ ¼"] = _df["ã‚»ãƒªå–å¼•ä¾¡æ ¼"].replace({"-": "0ä¸‡å††(nanå¹´nan)", np.nan: "nanä¸‡å††(nanå¹´nan)"})
    def f(x):
        return x[x.index("å¹´")+1: -1]
    _df["ã‚»ãƒªå–å¼•å ´æ‰€"] = _df["ã‚»ãƒªå–å¼•ä¾¡æ ¼"].map(f).astype("category")

    def f(x):
        return x[x.index("(")+1: x.index("å¹´")]
    _df["ã‚»ãƒªå–å¼•å¹´"] = _df["ã‚»ãƒªå–å¼•ä¾¡æ ¼"].map(f).astype(float)

    def f(x):
        return x[:x.index("ä¸‡å††")].replace("å„„", "").replace(",", "")
    _df["ã‚»ãƒªå–å¼•ä¾¡æ ¼"] = _df["ã‚»ãƒªå–å¼•ä¾¡æ ¼"].map(f).astype(float)


    def f(x):
        try:
            return x.replace("/result/recent", "")
        except:
            return np.nan

    for col_name, col_id in (("èª¿æ•™å¸«", "trainer_id"), ("é¦¬ä¸»", "owner_id"), ("ç”Ÿç”£è€…", "breeder_id"), ("æ¯æ¯_name", "æ¯æ¯_ped_id"), ("æ¯çˆ¶_name", "æ¯çˆ¶_ped_id"), ("æ¯_name", "æ¯_ped_id"), ("çˆ¶æ¯_name", "çˆ¶æ¯_ped_id"), ("çˆ¶çˆ¶_name", "çˆ¶çˆ¶_ped_id"), ("çˆ¶_name", "çˆ¶_ped_id")):
        _df[col_id] = _df[col_id].map(f)
        for name, id in _df[[col_name, col_id]].values:
            if id==id:
                nm[id] = name
        _df[col_id] = _df[col_id].astype("category")
        _df = _df.drop(columns=[col_name])
    
    _df["horse_id"] = _df["horse_id"].astype("category")
    _df = _df.drop(columns=["ä¸»ãªå‹é", "è¿‘è¦ªé¦¬", "é€šç®—æˆç¸¾", "ç²å¾—è³é‡‘", "horse_name"])


    return _df

def preprocess_form(_df):

    # ã„ã‚‰ãªã„ columns æ¶ˆå»
    _df = _df.drop(columns=["æ˜ åƒ", "é¦¬å ´æŒ‡æ•°", "ï¾€ï½²ï¾‘æŒ‡æ•°", "å©èˆï½ºï¾’ï¾ï¾„", "å‚™è€ƒ", "å‹ã¡é¦¬(2ç€é¦¬)"], errors="ignore")
    _df["æ—¥ä»˜"] = _df["æ—¥ä»˜"].fillna("2022/4/16").map(lambda x: datetime.datetime.strptime(x, "%Y/%m/%d"))

    def f(x):
        if x[0].isdecimal():
            return x[0]
        return "nan"
    _df["å›"] = _df["é–‹å‚¬"].fillna("nan").map(f).astype(float)
    assert  all(True if x in set(range(1,9)) else print(x) for x in _df["å›"].fillna(1))

    def f(x):
        if x[0].isdecimal():
            s = 1
        else:
            s = 0
        if x[-1].isdecimal():
            if x[-2].isdecimal():
                return x[s:-2]
            else:
                return x[s:-1]
        return x[s:]
    _df["é–‹å‚¬åœ°"] = _df["é–‹å‚¬"].fillna("nan").map(f).replace({"nan", np.nan}).astype("category")

    def f(x):
        if x[-1].isdecimal():
            if x[-2].isdecimal():
                return x[-2:]
            else:
                return x[-1:]
        return "nan"
    _df["æ—¥ç›®"] = _df["é–‹å‚¬"].fillna("nan").map(f).astype(float)
    assert set(_df["æ—¥ç›®"].fillna(1)) <= set(range(1,13))

    _df = _df.drop(columns=["é–‹å‚¬"])


    _df["å¤©æ°—"] = _df["å¤©æ°—"].astype("category")
    assert set(_df["å¤©æ°—"].astype("str")) <= {'å°é›¨', 'å°é›ª', 'æ™´', 'æ›‡', 'é›¨', 'é›ª', "nan"}

    _df["R"] = _df["R"].astype(float).replace({0: np.nan})
    assert all(True if x in set(range(1,14)) else print(x) for x in _df["R"].fillna(1))

    def f(x):
        if "æœªå‹åˆ©" in x:
            return "æœªå‹"
        if "æ–°é¦¬" in x:
            return "æ–°é¦¬"
        if ("500ä¸‡ä¸‹" in x) or ("1å‹ã‚¯ãƒ©ã‚¹" in x):
            return "1å‹"
        if ("1000ä¸‡ä¸‹" in x) or ("2å‹ã‚¯ãƒ©ã‚¹" in x):
            return "1å‹"
        if ("1600ä¸‡ä¸‹" in x) or ("3å‹ã‚¯ãƒ©ã‚¹" in x):
            return "3å‹"
        if "ãƒ•ã‚¡ã‚¤ãƒŠãƒ«" in x:
            return "Final"
        x = x.replace("ä¸€", "1").replace("äºŒ", "2").replace("ä¸‰", "3").replace("ã‚¤", "1").replace("ãƒ­", "2").replace("ãƒ", "3").replace("ãƒ¼", "")
        for key, value in (("(G1)", "G1"), ("(G2)", "G2"), ("(G3)", "G3"), ("(L)", "L"), ("(OP)", "OP"), ("OP", "OP_"), ("(G)", "G")):
            if key in x:
                return value

        for key in ("C1", "C2", "C3", "C4", "B1", "B2", "B3", "B4", "A1", "A2", "A3", "A4"):
            if key in x:
                return key
        return "other"
    _df["ã‚¯ãƒ©ã‚¹"] = _df["ãƒ¬ãƒ¼ã‚¹å"].fillna("other").map(f).astype("category")

    def f(x):
        for key, value in ([(f"{i}æ­³ä»¥", f"{i}+") for i in range(2,6)] + [(f"{i}æ­³", f"{i}") for i in range(2,6)]):
            if key in x:
                return value
        return "other"
    _df["å¹´é½¢åˆ¶é™"] = _df["ãƒ¬ãƒ¼ã‚¹å"].fillna("other").map(f).astype("category")

    _df["é ­æ•°"] = _df["é ­æ•°"].astype(float)
    assert set(_df["é ­æ•°"].fillna("nan")) <= set(range(0,40)) | set(["nan"]) 


    # æµ·å¤–ã€å†è€ƒã®ä½™åœ°ã‚ã‚Š
    _df["æ ç•ª"] = _df["æ ç•ª"].astype(float)
    assert set(_df["æ ç•ª"].fillna("nan")) <= set(range(1, 19)) | set(["nan"])
    _df["é¦¬ç•ª"] = _df["é¦¬ç•ª"].astype(float)
    assert set(_df["é¦¬ç•ª"].fillna("nan")) <= set(range(0,30)) | set(["nan"])

    _df["ã‚ªãƒƒã‚º"] = _df["ã‚ªãƒƒã‚º"].fillna("nan").astype(str).map(lambda x: x.replace(",", "")).astype(float).map(lambda x: np.nan if 0<=x<1 else x)
    assert all(1<=x<=10000 for x in _df["ã‚ªãƒƒã‚º"].fillna(9999) )

    _df["äººæ°—"] = _df["äººæ°—"].astype(float).map(lambda x: np.nan if x==0 else x)
    assert set(_df["äººæ°—"].fillna("nan")) <= set(range(1,40)) | set(["nan"])

    def f(x):
        if x[-2:] in ("é™)", "å†)"):
            return x[:-3]
        return x
    _df["ç€é †"] = _df["ç€é †"].astype(str).replace({x: np.nan for x in "å–ä¸­é™¤å¤±"}).fillna("nan").map(f).astype(float).replace({0:np.nan})
    assert set(_df["ç€é †"].fillna("nan")) <= set(range(1,30)) | set(["nan"])

    _df["æ–¤é‡"] = _df["æ–¤é‡"].fillna("nan").astype(float)
    assert all(40<=x<=75 for x in set(_df["æ–¤é‡"].fillna(50)))

    def f(x):
        if x[0] in "èŠãƒ€éšœ":
            return x[0]
        else:
            return np.nan
    _df["èŠãƒ€"] = _df["è·é›¢"].astype(str).map(f).astype("category")

    def f(x):
        if x[0] in "èŠãƒ€éšœ":
            if x[1:]:
                return x[1:]
            else:
                return np.nan
        else:
            return x
    _df["è·é›¢"] = _df["è·é›¢"].astype(str).map(f).astype(float)
    assert all(True if 800<=x<=10000 else print(x) for x in set(_df["è·é›¢"].fillna(1600)))

    def f(x):
        if x[0] in "èŠãƒ€éšœ":
            return x[0]
        else:
            return np.nan
    _df["é¦¬å ´"] = _df["é¦¬å ´"].astype("category")
    assert set(_df["é¦¬å ´"].astype(str)) <= set("è‰¯ç¨é‡ä¸")|set(["nan"])

    def f(x):
        x = x.replace(":", ".")
        if len(x)==6:
            time = int(x[0])*60 + float(x[3:])
        elif x.count(".") == 2:
            time = int(x[:x.index(".")])*60 + float(x[x.index(".")+1:])
        else:
            return np.nan
        
        if time < 20:
            return  np.nan
        return time

    _df["ã‚¿ã‚¤ãƒ "] = _df["ã‚¿ã‚¤ãƒ "].fillna("0:00.0").map(f)
    assert all(True if 30<=x<=700 else print(x) for x in set(_df["ã‚¿ã‚¤ãƒ "].fillna(100)))

    _df["ç€å·®"] = np.where( (_df["ç€å·®"].astype(float)>=0)|(_df["ç€å·®"].isnull()), _df["ç€å·®"].astype(float), 0)
    assert all(True if 0<=x<=150 else print(x) for x in set(_df["ç€å·®"].fillna(1)))
    
    _df["é€šé"] = _df["é€šé"].fillna("nan").map(lambda x: np.array(x.split("-"), dtype=float) )


    def f(x):
        i = x.find("-")
        if i == -1:
            return np.nan
        time = float(x[:i])
        if time < 10:
            return np.nan
        return time
    _df["race_ãƒ†ãƒ³"] = _df["ãƒšãƒ¼ã‚¹"].astype(str).map(f)
    assert all(True if 10<=x<=150 else print(x) for x in set(_df["race_ãƒ†ãƒ³"].fillna(32)))

    def f(x):
        i = x.find("-")
        if i == -1:
            return np.nan
        return x[i+1:]
    _df["race_ä¸Šã‚Š"] = _df["ãƒšãƒ¼ã‚¹"].astype(str).map(f).astype(float).replace({0: np.nan})
    assert all(True if 25<=x<=50 else print(x) for x in set(_df["race_ä¸Šã‚Š"].fillna(32)))
    _df = _df.drop(columns="ãƒšãƒ¼ã‚¹")


    _df["ä¸Šã‚Š"] = _df["ä¸Šã‚Š"].astype(float)
    # assert all(31<=x<=50 for x in set(_df["ä¸Šã‚Š"].fillna(32)))


    ### å…¥åŠ›ãƒŸã‚¹ã®ä¿®æ­£
    _df["é¦¬ä½“é‡"] = _df["é¦¬ä½“é‡"].replace({"-1(-454)": "454(+1)"})
    ###
    def f(x):
        i = x.find("(")
        if i == -1:
            return np.nan
        return x[4:-1]
    _df["å¢—æ¸›"] = _df["é¦¬ä½“é‡"].fillna("nan").map(f).astype(float)
    assert all(-100<=x<=100 for x in _df["å¢—æ¸›"].fillna(0))
    def f(x):
        return x[:3]
    _df["é¦¬ä½“é‡"] = _df["é¦¬ä½“é‡"].fillna("nan").replace({"è¨ˆä¸":"nan"}).map(f).astype(float)
    assert all(300<=x<=700 for x in _df["é¦¬ä½“é‡"].fillna(400))

    _df["è³é‡‘"] = _df["è³é‡‘"].fillna("0").replace({"": "0"}).map(lambda x: x.replace(",","")).astype(float)
    assert all(0<=x<=40000 for x in set(_df["è³é‡‘"]))


    # ID
    for col_name, col_id in (("ãƒ¬ãƒ¼ã‚¹å", "race_id"), ("é¨æ‰‹", "jockey_id")):
        _df[col_id] = _df[col_id].astype(str).map(lambda x: x.replace("/result/recent", "")).astype("category")
        for name, id in _df[[col_name, col_id]].values:
            nm[id] = name
        _df = _df.drop(columns=[col_name])


    return _df



def preprocess_v2(df_form):
    df_form["æ–¤é‡æ¯”"] = df_form["æ–¤é‡"] / df_form["é¦¬ä½“é‡"]
    df_form["å¢—æ¸›æ¯”"] = df_form["å¢—æ¸›"] / df_form["é¦¬ä½“é‡"]

    df_form["æ¨™æº–ã‚¿ã‚¤ãƒ "] = df_form["ã‚¿ã‚¤ãƒ "] / df_form["è·é›¢"]
    df_form["æ¨™æº–ç€å·®"] = df_form["ç€å·®"] / df_form["è·é›¢"]
    df_form.loc[df_form["race_ãƒ†ãƒ³"]<20, "race_ãƒ†ãƒ³"] = np.nan
    df_form.loc[df_form["race_ãƒ†ãƒ³"]>90, "race_ãƒ†ãƒ³"] /= 3
    df_form["race_ä¸Šã‚Šå·®"] = df_form["race_ä¸Šã‚Š"] - df_form["ä¸Šã‚Š"]

    df_form["é€šéå¹³å‡"] = df_form["é€šé"].map(lambda x: x.mean())
    df_form["æ¨™æº–é€šéå¹³å‡"] = df_form["é€šéå¹³å‡"] / df_form["é ­æ•°"]
    df_form.loc[df_form["æ¨™æº–é€šéå¹³å‡"]>1, "æ¨™æº–é€šéå¹³å‡"] = np.nan

    return df_form

def df_astype(_df_race, _df_result, _df_prof, _df_form):
    category_cols = ["å¤©æ°—", "é¦¬å ´", "race_id", "jockey_id", "horse_id", "é–‹å‚¬åœ°", "å¹´é½¢åˆ¶é™", "èŠãƒ€", "ã‚¯ãƒ©ã‚¹"]
    _df_form = _df_form.astype({col:"category" for col in category_cols})
    _df_form["æ—¥ä»˜"] = _df_form["æ—¥ä»˜"].map(lambda x: datetime.datetime.strptime(x, "%Y-%m-%d"))

    category_cols = ["horse_id", "jockey_id", "jockey_id", "owner_id", "race_id", "trainer_id", "æ€§", "æ±è¥¿"]
    _df_result = _df_result.astype({col:"category" for col in category_cols})

    category_cols = ["race_id", "å¤©æ°—", "é¦¬å ´", "èŠãƒ€", "æ—¥ç›®", "å›ã‚Š", "é–‹å‚¬åœ°", "ã‚¯ãƒ©ã‚¹", "å¹´é½¢åˆ¶é™"]
    _df_race = _df_race.astype({col:"category" for col in category_cols})
    _df_race["æ—¥ä»˜"] = _df_race["æ—¥ä»˜"].map(lambda x: datetime.datetime.strptime(x, "%Y-%m-%d"))
    category_cols = ["horse_id", "trainer_id", "owner_id", "breeder_id", "ç”£åœ°", "çˆ¶_ped_id", "çˆ¶çˆ¶_ped_id", "çˆ¶æ¯_ped_id", "æ¯_ped_id", "æ¯çˆ¶_ped_id", "æ¯æ¯_ped_id", "èª¿æ•™å ´æ‰€", "ã‚»ãƒªå–å¼•å ´æ‰€"]
    
    _df_prof = _df_prof.astype({col:"category" for col in category_cols})
    _df_prof["ç”Ÿå¹´æœˆæ—¥"] = _df_prof["ç”Ÿå¹´æœˆæ—¥"].map(lambda x: datetime.datetime.strptime(x, "%Y-%m-%d"))
    return _df_race, _df_result, _df_prof, _df_form

def get_shift_form(_df_form, n_shift=3):
    ## shift
    _df_form["key"] = _df_form["horse_id"]
    df_list = [_df_form.groupby(["key"]).shift(0)]
    for i in range(1, n_shift+1):
        df_shift = _df_form.groupby(["key"]).shift(-i).drop(columns="horse_id").add_suffix(f'_{i}')
        df_list.append(df_shift)

    ## agg
    df_agg = pd.DataFrame()
    _df_form["æ—¥ä»˜_float"] = (-_df_form["æ—¥ä»˜"].map(lambda x: x.timestamp())/(60*60*24)).astype(int)

    _df_form["1"] = 1
    _df_form["ã‚ªãƒƒã‚º_log10"] = np.log10(_df_form["ã‚ªãƒƒã‚º"])
    _df_form["å„ªå‹"] = (_df_form["ç€é †"]==1)
    _df_form["é€£å¯¾"] = (_df_form["ç€é †"]<=2)
    _df_form["è¤‡å‹"] = (_df_form["ç€é †"]<=3)

    ## ä¸è‰¯é¦¬å ´ã§ã®æˆç¸¾
    

    ## åŒé–‹å‚¬åœ°ã§ã®æˆç¸¾

    _df_form_groupby = _df_form.groupby(["horse_id"])
    df_agg["å‡ºèµ°é–“éš”"] = _df_form_groupby["æ—¥ä»˜_float"].diff().shift(-1)

    _df_form_groupby = _df_form.iloc[::-1].groupby(["horse_id"])
    _df_form["å‡ºèµ°å›æ•°"] = _df_form_groupby["1"].cumsum()
    df_agg["å‡ºèµ°å›æ•°"] = _df_form["å‡ºèµ°å›æ•°"]

    # ç´¯ç©å’Œ
    cumsum_cols = ["è³é‡‘", "å„ªå‹", "é€£å¯¾", "è¤‡å‹"]
    for col in cumsum_cols:
        _df_form[f"{col}_sum"] = _df_form_groupby[col].cumsum()
    # ç´¯ç©å¹³å‡
    average_cols = ["è³é‡‘", "å„ªå‹", "é€£å¯¾", "è¤‡å‹"]
    for col in average_cols:
        _df_form[f"{col}_ave"] = _df_form_groupby[col].cumsum() / df_agg["å‡ºèµ°å›æ•°"]
    ## leak_colã‚’-1ã‚·ãƒ•ãƒˆ
    leak_cols = []
    leak_cols += [f"{col}_sum" for col in cumsum_cols]
    leak_cols += [f"{col}_ave" for col in average_cols]
    _df_form_groupby = _df_form.groupby(["horse_id"])
    for col in leak_cols:
        df_agg[col] = _df_form.groupby(["horse_id"])[col].shift(-1).fillna(0)

    df_list.append(df_agg)

    return pd.concat(df_list, axis=1)

def get_shift_form_v2(_df_form, n_shift=3):
    ## shift
    _df_form["key"] = _df_form["horse_id"]
    df_list = [_df_form.groupby(["key"]).shift(0)]
    for i in range(1, n_shift+1):
        df_shift = _df_form.groupby(["key"]).shift(-i).drop(columns="horse_id").add_suffix(f'_{i}')
        df_list.append(df_shift)

    ## agg
    df_agg = pd.DataFrame()
    _df_form["æ—¥ä»˜_float"] = (-_df_form["æ—¥ä»˜"].map(lambda x: x.timestamp())/(60*60*24)).astype(int)


    ## å‡ºèµ°é–“éš”
    _df_form_groupby = _df_form.groupby(["horse_id"])
    df_agg["å‡ºèµ°é–“éš”"] = _df_form_groupby["æ—¥ä»˜_float"].diff().shift(-1)

    conditions =  [
                ["", np.ones(_df_form.shape[0]).astype(bool)], 
                ["é¦¬å ´ç¨~ä¸_", _df_form["é¦¬å ´"]!="è‰¯"],
                #    ["S_", _df_form["è·é›¢"]<=1599],
                #    ["M_", (_df_form["è·é›¢"]>=1600) & (_df_form["è·é›¢"]<=1899)],
                #    ["I_", (_df_form["è·é›¢"]>=1900) & (_df_form["è·é›¢"]<=2100)],
                #    ["L_", (_df_form["è·é›¢"]>=2101) & (_df_form["è·é›¢"]<=2700)],
                #    ["E_", (_df_form["è·é›¢"]>=2701)],
                ]
    
    # conditions += [
    #                 [f"{kaisaichi}_", _df_form["é–‹å‚¬åœ°"]==kaisaichi] 
    #                             for kaisaichi in ["æ±äº¬", "ä¸­å±±", "äº¬éƒ½", "é˜ªç¥"]
    #                ]

    for key, condition in conditions:

        _df_form0 = _df_form.copy()
        _df_form0["1"] = 1 * condition
        _df_form0["å„ªå‹"] = (_df_form["ç€é †"]==1) * condition
        _df_form0["é€£å¯¾"] = (_df_form["ç€é †"]<=2) * condition
        _df_form0["è¤‡å‹"] = (_df_form["ç€é †"]<=3) * condition
        _df_form0["è³é‡‘"] = _df_form["è³é‡‘"] * condition

        _df_form0_groupby = _df_form0.iloc[::-1].groupby(["horse_id"])
        _df_form0["å‡ºèµ°å›æ•°"] = _df_form0_groupby["1"].cumsum()
        df_agg[f"{key}å‡ºèµ°å›æ•°"] = _df_form0["å‡ºèµ°å›æ•°"]

        # ç´¯ç©å’Œ
        cumsum_cols = ["è³é‡‘", "å„ªå‹", "é€£å¯¾", "è¤‡å‹"]
        for col in cumsum_cols:
            _df_form0[f"{col}_sum"] = _df_form0_groupby[col].cumsum()
        # ç´¯ç©å¹³å‡
        average_cols = ["è³é‡‘", "å„ªå‹", "é€£å¯¾", "è¤‡å‹", "æ¨™æº–ç€å·®", "æ¨™æº–é€šéå¹³å‡", "æ¨™æº–ã‚¿ã‚¤ãƒ ", "ã‚ªãƒƒã‚º", "ä¸Šã‚Š"]
        for col in average_cols:
            _df_form0[f"{col}_ave"] = (_df_form0_groupby[col].cumsum() / df_agg[f"{key}å‡ºèµ°å›æ•°"]).replace({np.inf: np.nan})
        ## leak_colã‚’-1ã‚·ãƒ•ãƒˆ
        leak_cols = []
        leak_cols += [f"{col}_sum" for col in cumsum_cols]
        leak_cols += [f"{col}_ave" for col in average_cols]
        _df_form0_groupby = _df_form0.groupby(["horse_id"])
        for col in leak_cols:
            df_agg[f"{key}{col}"] = _df_form0.groupby(["horse_id"])[col].shift(-1).fillna(np.nan)

    df_list.append(df_agg)

    return pd.concat(df_list, axis=1)

def df_concat(_df_result, _df_race, _df_prof, _df_form):
    _df_result = pd.merge(_df_result, _df_race, how='left', on="race_id")
    _df_result = pd.merge(_df_result, _df_prof.drop(columns=["trainer_id", "owner_id"]), how='left', on="horse_id")
    drop_colls = ["ç€é †", "æ ç•ª", "é¦¬ç•ª", "æ–¤é‡", "ã‚¿ã‚¤ãƒ ", "é€šé", "ä¸Šã‚Š", "ã‚ªãƒƒã‚º", "äººæ°—", "è³é‡‘", "jockey_id", "é¦¬ä½“é‡", "å¢—æ¸›", "R", "å¤©æ°—", "é¦¬å ´", "æ—¥ä»˜", "èŠãƒ€", "è·é›¢", "å›", "é–‹å‚¬åœ°", "æ—¥ç›®", "å¹´é½¢åˆ¶é™", "ã‚¯ãƒ©ã‚¹", "race_ãƒ†ãƒ³", "race_ä¸Šã‚Š"]
    _df_result = pd.merge(_df_result, _df_form.drop(columns=drop_colls), how='left', on=["race_id", "horse_id"])
    return _df_result

def get_pp2(_df):
    _df["ä¹—ã‚Šæ›¿ã‚ã‚Š"] = (_df["jockey_id"].astype(str) != _df["jockey_id_1"].astype(str)).value_counts()
    _df["è·é›¢å·®"] = (_df["è·é›¢"] - _df["è·é›¢_1"])
    _df["è·é›¢æ¯”"] = _df["è·é›¢"] / _df["è·é›¢_1"]

    _df["å¹´é½¢flaot"] = (_df["æ—¥ä»˜"] - _df["ç”Ÿå¹´æœˆæ—¥"]).map(lambda x: x.days)/365

    for i in range(1, n_shift+1):
        _df[f"ã‚ªãƒƒã‚º_log_{i}"] = _df[f"ã‚ªãƒƒã‚º_{i}"]
    for col in ["ã‚ªãƒƒã‚º", "ã‚ªãƒƒã‚º_log", "äººæ°—", "ç€å·®", "ç€é †", "æ¨™æº–ç€å·®", "æ¨™æº–ã‚¿ã‚¤ãƒ ", "ä¸Šã‚Š", "é€šéå¹³å‡", "race_ä¸Šã‚Šå·®"]:
        _df[f"{col}_{n_shift}_ave"] = np.nanmean(_df[[f"{col}_{i}" for i in range(1, n_shift+1)]], axis=1)
        _df[f"{col}_{n_shift}_ave"] = np.mean(_df[[f"{col}_{i}" for i in range(1, n_shift+1)]], axis=1)
    return _df

def encoded(_df, upload=False, download=False):
    if download:
#         %cd /content/drive/My Drive/Horse Racing/models
        category_cols = pickle.load(open("category_cols.sav", 'rb'))
        print("GET OrdinalEncoder, 'oe.sav' !!")
        oe = pickle.load(open("oe.sav", 'rb'))
        _df[category_cols] = oe.transform(_df[category_cols].values)
        _df[category_cols] = _df[category_cols].astype('category')
    else:
        category_cols = [col for col in _df.columns if _df[col].dtype.name == "category"]
        oe = preprocessing.OrdinalEncoder(
                handle_unknown = 'use_encoded_value',
                unknown_value = np.nan,
        )
        _df[category_cols] = oe.fit_transform(_df[category_cols].values)
        _df[category_cols] = _df[category_cols].astype('category')

    if upload:
#         %cd /content/drive/My Drive/Horse Racing/models
        pickle.dump(category_cols, open(f'category_cols.sav', 'wb'))
        print("LOAD OrdinalEncoder, 'oe.sav' !!")
        pickle.dump(oe, open(f'oe.sav', 'wb'))

    return _df

# My LightGBM

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def softmax(x):
    f = np.exp(x)/np.sum(np.exp(x), axis = 1, keepdims = True)
    return f

def focal_loss(x: np.ndarray, t: np.ndarray, gamma: float=1):
    """
    Params::
        x:
            äºˆæ¸¬å€¤. è¦æ ¼åŒ–ã•ã‚Œã¦ã„ãªã„å€¤ã§ã‚ã‚‹ã“ã¨. 1æ¬¡å…ƒ
            >>> x
            array([0.65634349, 0.8510698 , 0.61597224])
            multi classã®ã¨ã. ä¾‹ãˆã° 3 class ã®å ´åˆã¯ä¸‹è¨˜ã®ã‚ˆã†ãªå€¤ã«ãªã£ã¦ã„ã‚‹ã“ã¨.
            >>> x
            array([
                [0.65634349, 0.8510698 , 0.61597224],
                [0.58012161, 0.79659195, 0.39168051]
            ])
        t: (n_class, ) 1æ¬¡å…ƒé…åˆ—
    """
    t = t.astype(np.int32)
    if len(x.shape) > 1:
        x = softmax(x)
        t = np.identity(x.shape[1])[t]
        return -1 * t * (1 - x)**gamma * np.log(x)        
    else:
        x = sigmoid(x)
        x[t == 0] = 1 - x[t == 0] # 0ãƒ©ãƒ™ãƒ«ç®‡æ‰€ã¯ç¢ºç‡ã‚’åè»¢ã™ã‚‹
        return -1 * (1 - x)**gamma * np.log(x)

def focal_loss_grad(x: np.ndarray, t: np.ndarray, gamma: float=1):
    """
    å†…éƒ¨ã« softmax ã‚’å«ã‚€é–¢æ•°ã«ã¤ã„ã¦ã¯ derivative ã§ã¯è¨ˆç®—ãŒå®‰å®šã—ãªã„.
    """
    t = t.astype(np.int32)
    if len(x.shape) > 1:
        x = softmax(x) # softmax ã§è¦æ ¼åŒ–
        # æ­£è§£åˆ—ã‚’æŠœãå‡ºã—
        xK = x[np.arange(t.shape[0]).reshape(-1, 1), t.reshape(-1, 1)]
        xK = np.tile(xK, (1, x.shape[1]))
        # x1 ã¯ ä¸æ­£è§£åˆ—ã« -1 ã‚’ã‹ã‘ã¦ã€ã•ã‚‰ã«æ­£è§£åˆ—ã¯ãã“ã‹ã‚‰1ã‚’è¶³ã™æ“ä½œ
        x1 = x.copy()
        x1 = -1 * x1
        x1[np.arange(t.shape[0]).reshape(-1, 1), t.reshape(-1, 1)] = x1[np.arange(t.shape[0]).reshape(-1, 1), t.reshape(-1, 1)] + 1
        dfdy = gamma * (1 - xK) ** (gamma-1) * np.log(xK) - ((1 - xK) ** gamma / xK)
        dydx = xK * x1
        grad = dfdy * dydx

        dfdydx = dydx * (2 * gamma * (1 - xK) ** (gamma - 1) / xK - gamma * (gamma - 1) * np.log(xK) * (1 - xK) ** (gamma - 2) + (1 - xK) ** gamma * (xK ** -2))
        dydxdx = dydx * (1 - 2 * x)
        hess = dfdy * dydxdx + dydx * dfdydx
    else:
        grad = derivative(lambda _x: focal_loss(_x, t, gamma=gamma), x, n=1, dx=1e-6)
        hess = derivative(lambda _x: focal_loss(_x, t, gamma=gamma), x, n=2, dx=1e-6)

    return grad, hess

def lgb_custom_objective(y_pred: np.ndarray, data: lgbm.Dataset, func_loss, is_lgbdataset: bool=True):
    """
    lightGBMã®customized objectiveã®å…±é€šé–¢æ•°
    Params::
        y_pred:
            äºˆæ¸¬å€¤. multi classã®å ´åˆã¯ã€n_sample * n_class ã®é•·ã•ã«ãªã£ãŸã„ã‚‹
            å€¤ã¯ã€array([0ãƒ‡ãƒ¼ã‚¿ç›®0ãƒ©ãƒ™ãƒ«ã®äºˆæ¸¬å€¤, ..., Nãƒ‡ãƒ¼ã‚¿ç›®0ãƒ©ãƒ™ãƒ«ã®äºˆæ¸¬å€¤, 0ãƒ‡ãƒ¼ã‚¿ç›®1ãƒ©ãƒ™ãƒ«ã®äºˆæ¸¬å€¤, ..., ])
        data:
            train_set ã« set ã—ãŸå€¤
        func_loss:
            y_pred, y_true ã‚’å…¥åŠ›ã«æŒã¡ã€y_pred ã¨åŒã˜ shape ã‚’æŒã¤ return ã‚’ã™ã‚‹
        is_lgbdataset:
            lgb.dataset ã§ãªã‹ã£ãŸå ´åˆã¯å…¥åŠ›ãŒé€†è»¢ã™ã‚‹ã®ã§æ°—ã‚’ã¤ã‘ã‚‹
    """
    if is_lgbdataset == False:
        y_true = y_pred.copy()
        y_pred = data
    else:
        y_true = data.label
    if y_pred.shape[0] != y_true.shape[0]:
        # multi class ã®å ´åˆ
        n_class = int(y_pred.shape[0] / y_true.shape[0])
        y_pred = y_pred.reshape(n_class, -1).T
    grad, hess = func_loss(y_pred, y_true)
    return grad.T.reshape(-1), hess.T.reshape(-1)

def lgb_custom_eval(y_pred: np.ndarray, data: lgbm.Dataset, func_loss, func_name: str, is_higher_better: bool, is_lgbdataset: bool=True):
    """
    lightGBMã®customized objectiveã®å…±é€šé–¢æ•°
    Params::
        y_pred:
            äºˆæ¸¬å€¤. multi classã®å ´åˆã¯ã€n_sample * n_class ã®é•·ã•ã«ãªã£ãŸã„ã‚‹
            å€¤ã¯ã€array([0ãƒ‡ãƒ¼ã‚¿ç›®0ãƒ©ãƒ™ãƒ«ã®äºˆæ¸¬å€¤, ..., Nãƒ‡ãƒ¼ã‚¿ç›®0ãƒ©ãƒ™ãƒ«ã®äºˆæ¸¬å€¤, 0ãƒ‡ãƒ¼ã‚¿ç›®1ãƒ©ãƒ™ãƒ«ã®äºˆæ¸¬å€¤, ..., ])
        data:
            train_set ã« set ã—ãŸå€¤
        func_loss:
            y_pred, y_true ã‚’å…¥åŠ›ã«æŒã¡ã€grad, hess ã‚’ return ã™ã‚‹é–¢æ•°
    """
    if is_lgbdataset == False:
        y_true = y_pred.copy()
        y_pred = data
    else:
        y_true  = data.label
    n_class = 1
    if y_pred.shape[0] != y_true.shape[0]:
        # multi class ã®å ´åˆ
        n_class = int(y_pred.shape[0] / y_true.shape[0])
        y_pred = y_pred.reshape(n_class, -1).T
    value = func_loss(y_pred, y_true)
    return func_name, np.sum(value), is_higher_better

f   = lambda x, y: focal_loss(x, y, gamma=1.0)
f_g = lambda x, y: focal_loss_grad(x, y, gamma=1.0)

class MyLGBMClassifier(lgbm.LGBMClassifier):
    """
    custom objective ã‚’æƒ³å®šã—ã¦å€¤ã‚’è¦æ ¼åŒ–ã§ãã‚‹ã‚ˆã†ã«è‡ªä½œclassã‚’å®šç¾©ã™ã‚‹
    """
    def predict_proba(self, X, *argv, **kwargs):
        proba = super().predict_proba(X, *argv, **kwargs)
        if len(proba.shape) == 2:
            proba = softmax(proba)
        else:
            proba = sigmoid(proba)
            proba[:, 0] = 1 - proba[:, 1]
        return proba

def model_fit(model_name, X_train, y_train, X_valid, y_valid, **params):
    """
    return score(float), model, pred(arr)
    """
    is_binary = all(yi in {1,0} for yi in y_valid)
    if is_binary:
        if model_name == "LightGBM":
            model = lgbm.LGBMClassifier(
                                **params,
                                n_estimators=10000, 
                                silent=1
                                )        
            model.fit(
                X_train, y_train, 
                eval_set=[(X_valid, y_valid)],  
                early_stopping_rounds=50,
                eval_metric='auc',
                feature_name='auto', 
                categorical_feature = 'auto',
                verbose=False,
                )

            pred = model.predict_proba(X_valid, num_iteration=model.best_iteration_)[:, 1]
            score = roc_auc_score(y_valid, pred)
        elif model_name == "MyLightGBM":
            model = MyLGBMClassifier(
                                **params, 
                                n_estimators=10000, 
                                silent=1,
                                objective=(lambda x,y: lgb_custom_objective(x, y, f_g, is_lgbdataset=False)),
                                )
            model.fit(
                X_train, y_train, 
                eval_set=[(X_valid, y_valid)],  
                early_stopping_rounds=50,
                eval_metric='auc',
                feature_name='auto', 
                categorical_feature = 'auto',
                verbose=False,
                )

            pred = model.predict_proba(X_valid, num_iteration=model.best_iteration_)[:, 1]
            score = roc_auc_score(y_valid, pred)
        elif model_name == "CatBoost":
            model = CatBoostClassifier(
                    **params, 
                    early_stopping_rounds=50,
                    n_estimators=10000, 
                    silent=1,
                    )
            train_pool = Pool(X_train, y_train)
            valid_pool = Pool(X_valid, y_valid)

            model.fit(train_pool, eval_set=valid_pool)
            pred = model.predict_proba(X_valid, num_iteration=model.best_iteration_)[:, 1]
            # model.predict(X_test)
            score = roc_auc_score(y_valid, pred)
        else:
            print("is_not_exist")
    else:
        if model_name == "MyLightGBM":
            model_name = "LightGBM"
        if model_name == "LightGBM":
            model = lgbm.LGBMRegressor(
                            objective='regression',
                            **params,
                            n_estimators=10000, 
                            silent=1
                        ) 
            model.fit(
                X_train, y_train, 
                eval_set=[(X_valid, y_valid)],  
                early_stopping_rounds=50,
                # eval_metric='rmse',
                feature_name='auto', 
                categorical_feature = 'auto',
                verbose=False,
                )
            pred = model.predict(X_valid, num_iteration=model.best_iteration_)
            score = mean_squared_error(y_valid, pred)
        else:
            print("is_not_exist")

    return score, model, pred

def arr_dev(df_valid_, TARGET_pred, order="infer"):
    mean = np.array(df_valid_[["race_id", TARGET_pred]].groupby("race_id").mean().loc[df_valid_["race_id"], TARGET_pred])
    std = np.array(df_valid_[["race_id", TARGET_pred]].groupby("race_id").std().loc[df_valid_["race_id"], TARGET_pred])
    
    arr = 50 + (df_valid_[TARGET_pred] - mean) / std * 10

    reverse = False
    if order == "infer":
        if np.corrcoef(arr, df_valid_["ç€é †"])[0,1]>0:
            arr = 100 - arr
            reverse = True

    if order == "reverse":
        arr = 100 - arr
        reverse = True
    return arr, reverse

def get_new_race_csv(race_id_list):
    race_list = []
    result_list = []
    def add_new_race_info(race_id):
        url=f"https://race.netkeiba.com/race/shutuba.html?race_id={race_id}&rf=race_submenu"
        res = requests.get(url)
        res.encoding = "EUC-JP"
        soup = BeautifulSoup(res.text, "html.parser")

        # race
        race_soup = soup.find("div", attrs={"class": "RaceList_NameBox"})
        d = {}
        d["race_id"] = f"/race/{race_id}/"
        d["ãƒ¬ãƒ¼ã‚¹å"] = race_soup.find("div", class_="RaceName").text.split()[0]    
        grade = ""
        for key, g in [(1,"G1"), (2,"G2"), (3,"G3"), (5,"OP"), (15, "L"), (16, "3å‹"), (17, "2å‹"), (18, "1å‹")]:
            if soup.find("span", class_=f"Icon_GradeType Icon_GradeType{key}"):
                grade = g
        if g:
            d["ãƒ¬ãƒ¼ã‚¹å"] += f"({grade})"
        d["R"] = race_soup.find("span", class_="RaceNum").text[:-1]
        l = race_soup.find("div", class_="RaceData01").text.replace(" ", "").replace("\n", "").split("/")
        if len(l)==2:
            l += [":æ™´", ":è‰¯"]

        d["ã‚³ãƒ¼ã‚¹"] = l[1][0] + l[1][l[1].index("(")+1: l[1].index(")")] + l[1][1: l[1].index("(")]
        d["å¤©æ°—"] = l[2][l[2].index(":")+1:]
        d["é¦¬å ´"] = l[3][l[3].index(":")+1:].replace("ç¨", "ç¨é‡").replace("ä¸", "ä¸è‰¯")
        d["æ™‚é–“"] = l[0][:l[0].index("ç™º")]
        d["æ—¥ä»˜"] = np.nan
        l = race_soup.find("div", class_="RaceData02").text.split()
        d["é–‹å‚¬"] = l[0] + l[1] + l[2]
        d["æ¡ä»¶"] = l[3].replace("ã‚µãƒ©ç³»", "") + l[4]
        d["ã‚¿ã‚¤ãƒ—"] = "(".join(l[5:-2])
        d["éå»ãƒ¬ãƒ¼ã‚¹url"] = np.nan
        d["ãƒ¬ãƒ¼ã‚¹ç·ç§°"] = race_soup.find("div", class_="RaceName").text.split()[0]
        for col in [f"{i+1}ã‚³ãƒ¼ãƒŠãƒ¼" for i in range(4)] + ["ãƒ©ãƒƒãƒ—", "ãƒšãƒ¼ã‚¹"]:
            d[col] = np.nan
        d["ãƒ©ãƒƒãƒ—"] = d["ãƒšãƒ¼ã‚¹"] = "(nan-nan)"
        race_list.append(d)

        ## result
        shutuba_soups = soup.find("div", class_="RaceTableArea").find("table", class_="Shutuba_Table RaceTable01 ShutubaTable").find_all("tr")
        columns = [i.text.split()[0] for i in shutuba_soups[0].find_all("th")] + ["-"] + [f"{type_}_id" for type_ in ("horse", "jockey", "trainer")]
        shutuba_data = []
        # id_manage.add("race", race_id, race_soup.find("h1").text, True)
        data = [line for line in shutuba_soups[2:]]

        for line in data:
            shutuba_data.append([x.text.replace("\n", "") for x in line.find_all("td")]+[None]*3)   # ï¼‘è¡Œãƒ‡ãƒ¼ã‚¿ã‚’åŠ ãˆã‚‹
            for text in line.find_all("a"):
                id = text.get("href")
                name = text.get("title")
                if id:
                    for i, type_ in enumerate(("horse", "jockey", "trainer")):
                        if f"/{type_}/" in id:
                            shutuba_data[-1][13+i] = id.replace("https://db.netkeiba.com", "").replace("/result/recent", "")
                            # id_manage.add(type_, id, name, False)
                            # id_data.append([type_, id, name, False])
        _df = pd.DataFrame(data=shutuba_data, columns=columns)\
                    .drop(columns=["å°", "ãŠæ°—ã«å…¥ã‚Šé¦¬", "-", "äººæ°—", "æ›´æ–°"])\
                    .rename(columns={"æ ": "æ ç•ª", "é¦¬ä½“é‡(å¢—æ¸›)": "é¦¬ä½“é‡"})
        _df['race_id'] = f"/race/{race_id}/"

        _df["é¦¬ä½“é‡"] = _df["é¦¬ä½“é‡"].fillna("è¨ˆä¸").replace({"nan":"è¨ˆä¸"})

        # äº‹å¾Œ
        _df[['ç€é †','ã‚¿ã‚¤ãƒ ','ç€å·®','ï¾€ï½²ï¾‘æŒ‡æ•°','é€šé','ä¸Šã‚Š','å©èˆï½ºï¾’ï¾ï¾„','å‚™è€ƒ','è³é‡‘(ä¸‡å††)']] = np.nan
        # 1é€±é–“å‰
        # _df[["æ ç•ª", "é¦¬ç•ª"]] = np.nan
        # å¤‰å‹•
        _df[['å˜å‹','äººæ°—',]] = np.nan
        # _df['å˜å‹'] = [190.8,111.6,182.3,3.7,20.4,1.5,60.2,58.7,47.5,27.4,74.5,298.8,45.5,9.2,92.9,145.9]
        # _df['äººæ°—'] = [15,12,14,2,4,1,9,8,7,5,10,16,6,3,11,13]
        # å…¥æ‰‹å¯èƒ½
        _df["èª¿æ•™å¸«"] = _df["å©èˆ"].map(lambda x: x.replace("ç¾æµ¦", "[æ±]").replace("æ —æ±", "[è¥¿]").replace("åœ°æ–¹", "[åœ°]"))
        _df[['èª¿æ•™ï¾€ï½²ï¾‘', 'é¦¬ä¸»', 'owner_id']] = np.nan

        _df = _df.drop(columns=["å©èˆ"])
        result_list.append(_df)
    
    n = len(race_id_list)
    xxx = st.text(f"scrape {0}/{n} races")
    bar = st.progress(0.0)
    for i, race_id in enumerate(race_id_list):
        add_new_race_info(race_id)
        xxx.text(f"scrape {i+1}/{n} races")
        bar.progress((i+1)/n)
    df_race = pd.DataFrame(race_list).replace({"": np.nan, "--": np.nan})
    df_result = pd.concat(result_list, axis=0).replace({"": np.nan, "--": np.nan})
    xxx = st.empty()
    bar = st.empty()
    return df_race, df_result

def get_new_horse_csv(df_result):
    prof_list = []
    form_list = []
    def add_new_horse_info(horse_id, race_id):
        time.sleep(0.5)
        url=f"https://db.netkeiba.com{horse_id}"
        res = requests.get(url)
        res.encoding="EUC-JP"
        soup = BeautifulSoup(res.text, "html.parser")

        # [dict] profile, ped
        d = {}
        d["horse_id"] = horse_id
        d["horse_name"] = soup.find("div", attrs={"class": "horse_title"}).find("h1").text
        for line in soup.find("table", attrs={"class": "db_prof_table", "summary": "ã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«"}).find_all("tr"):
            d[line.th.text] = ''.join(line.td.text.split())
            for text in line.find_all("a"):
                id = text.get("href")
                name = text.get("title")
                if id:
                    for i, type_ in enumerate(("trainer", "owner", "breeder")):
                        if f"/{type_}/" in id:
                            d[f"{type_}_id"] = id
        ped_soups = soup.find("table", attrs={"class": "blood_table"}).find_all("td")
        for line, relative in zip(ped_soups, ("çˆ¶", "çˆ¶çˆ¶", "çˆ¶æ¯", "æ¯", "æ¯çˆ¶", "æ¯æ¯")):
            name = "".join(line.text.split())
            ped_id = line.find("a").get("href")
            d[f"{relative}_ped_id"] = ped_id
            d[f"{relative}_name"] = name
        prof_list.append(d)

        # [df] form
        tabel_data = soup.find("table", attrs={"class": "db_h_race_results nk_tb_common"})
        if tabel_data:
            form_soups = tabel_data.find_all("tr")
            columns = [column.text for column in form_soups[0].find_all("th")] + [f"{type_}_id" for type_ in ("race", "jockey", "horse")]
            form_data = []
            data = [line for line in form_soups[1:]]
            for line in data:
                form_data.append(["".join(x.text.split()) for x in line.find_all("td")]+[None]*2+[horse_id])
                for text in line.find_all("a"):
                    id = text.get("href")
                    name = text.get("title")
                    if id:
                        for i, type_ in enumerate(("race", "jockey")):
                            if name and (f"/{type_}/" in id) and (f"/{type_}/movie" not in id):
                                form_data[-1][28+i] = id
        else:
            form_data = [[]]
            columns = []

        d = {
            'ç€å·®': np.nan,
            'race_id': race_id,
            'horse_id': horse_id
        }
        form_list.append(
            pd.concat([pd.DataFrame([d]), pd.DataFrame(data=form_data, columns=columns)], axis=0).reset_index()
        )
    n = df_result.shape[0]
    xxx = st.text(f"scrape {n} horses")
    bar = st.progress(0.0)
    for i, (horse_id, race_id) in enumerate(df_result[["horse_id", "race_id"]].values):
        add_new_horse_info(horse_id, race_id)
        xxx.text(f"scrape {i+1}/{n} horses")
        bar.progress((i+1)/n)
    df_prof = pd.DataFrame(prof_list).replace({"": np.nan, "--": np.nan})
    df_form = pd.concat(form_list, axis=0).replace({"": np.nan, "--": np.nan})
    xxx = st.empty()
    bar = st.empty()

    # formãŒç©ºã®å ´åˆã«å¯¾å¿œï¼ˆæ–°é¦¬æˆ¦ã®ã¿ã®é¸æŠï¼‰
    cols = set(['index', 'ç€å·®', 'race_id', 'horse_id', 'æ—¥ä»˜', 'é–‹å‚¬', 'å¤©æ°—', 'R', 'ãƒ¬ãƒ¼ã‚¹å', 'æ˜ åƒ', 'é ­æ•°', 'æ ç•ª', 'é¦¬ç•ª', 'ã‚ªãƒƒã‚º', 'äººæ°—', 'ç€é †', 'é¨æ‰‹', 'æ–¤é‡', 'è·é›¢', 'é¦¬å ´', 'é¦¬å ´æŒ‡æ•°', 'ã‚¿ã‚¤ãƒ ', 'ï¾€ï½²ï¾‘æŒ‡æ•°', 'é€šé', 'ãƒšãƒ¼ã‚¹', 'ä¸Šã‚Š', 'é¦¬ä½“é‡', 'å©èˆï½ºï¾’ï¾ï¾„', 'å‚™è€ƒ', 'å‹ã¡é¦¬(2ç€é¦¬)', 'è³é‡‘', 'jockey_id'])
    df_form_cols = set(df_form.columns)
    for col in cols-df_form_cols:
        df_form[col] = np.nan
    return df_prof, df_form

def encoded(_df, upload=False, download=False):
    if download:
        category_cols = pickle.load(open("category_cols.sav", 'rb'))
        print("GET OrdinalEncoder, 'oe.sav' !!")
        oe = pickle.load(open("oe.sav", 'rb'))
        _df[category_cols] = oe.transform(_df[category_cols].values)
        _df[category_cols] = _df[category_cols].astype('category')
    else:
        category_cols = [col for col in _df.columns if _df[col].dtype.name == "category"]
        oe = preprocessing.OrdinalEncoder()
        _df[category_cols] = oe.fit_transform(_df[category_cols].values)
        _df[category_cols] = _df[category_cols].astype('category')

    if upload:
        pickle.dump(category_cols, open(f'category_cols.sav', 'wb'))
        print("LOAD OrdinalEncoder, 'oe.sav' !!")
        pickle.dump(oe, open(f'oe.sav', 'wb'))

    return _df

def get_held_races(date):
    url=f"https://yoso.netkeiba.com/?pid=race_list&kaisai_date={date}"
    res = requests.get(url)
    res.encoding = "EUC-JP"
    soup = BeautifulSoup(res.text, "html.parser")
    soups = soup.find_all("div", attrs={"class": "RaceList"})#.find("div", attrs={"class": "Jyo"})
    data = []
    for soup_i in soups:
        d = {}    
        headder1 = soup_i.find(class_="Jyo").text
        # headder2 = soup_i.find("ul", class_="JyoData")
        d["é–‹å‚¬åœ°"] = headder1[:2]
        d["å›"] = headder1[headder1.index("(")+1:headder1.index("å›")]
        d["æ—¥ç›®"] = headder1[headder1.index("å›")+1:headder1.index("æ—¥ç›®")]

        for race_i in soup_i.find("div", class_="RaceList_Main").find_all("li"):
            text = race_i.text.replace("\n","")
            R = int(text[:text.index("R")])
            url = race_i.find("a").get("href")
            name = text[text.index("R")+1: text.find(":")-2]
            race_id = re.sub(r"\D", "", url)

            d[R] = {}
            d[R]["R"] = R
            d[R]["name"] = name
            d[R]["race_id"] = race_id
        data.append(d)
    return data


    

## æ—¥ä»˜ã‚’é¸ã¶
if "data" not in st.session_state:
    today = datetime.date.today().isocalendar()
    year = today[0]
    week = today[1]

    sat = datetime.date.fromisocalendar(year, week, 6)
    sun = datetime.date.fromisocalendar(year, week, 7)

    st.write("æ—¥ä»˜ã®é¸æŠ")
    box_sat = st.checkbox(f'{str(sat).replace("-", "/")} (åœŸ)')
    box_sun = st.checkbox(f'{str(sun).replace("-", "/")} (æ—¥)')

    date_list = []
    click1 = st.button("OK")
    if click1:
        if box_sat:
            date_list.append(
                str(sat).replace("-", "")
            )
        if box_sun:
            date_list.append(
                str(sun).replace("-", "")
            )
        st.session_state["data"] = []
        for date in date_list:
            st.session_state["data"] += get_held_races(date)
        st.button("æ¬¡ã¸")

# é–‹å‚¬ã‚’é¸ã¶
elif "data2" not in st.session_state:
    st.write("é–‹å‚¬åœ°ã®é¸æŠ")
    options = [st.checkbox(f'{data_i["é–‹å‚¬åœ°"]}{data_i["å›"]}å›{data_i["æ—¥ç›®"]}æ—¥ç›®') 
                                                        for data_i in st.session_state["data"]]
    click2 = st.button("OK")
    if click2:
        st.session_state["data2"] = []
        for i, is_ok in enumerate(options):
            if is_ok:
                st.session_state["data2"].append(st.session_state["data"][i])
        st.button("æ¬¡ã¸")
        

# ãƒ¬ãƒ¼ã‚¹ã‚’é¸ã¶
elif "id_list" not in st.session_state:
    options = []
    id_list = []
    names = []
    columns = st.columns(len(st.session_state["data2"]))

    for i, (data_i, emoji) in enumerate(zip(st.session_state["data2"], "ğŸ‡ğŸ´ğŸğŸ† ã€€")):
        with columns[i]:
            st.header(f'{data_i["é–‹å‚¬åœ°"]}{data_i["å›"]}å›{data_i["æ—¥ç›®"]}æ—¥ç›®')
            races = [i for i in data_i.keys() if isinstance(i, int)]
            options += [st.checkbox(f'{i}R {data_i[i]["name"]} {emoji}') for i in races]
            id_list += [f'{data_i[i]["race_id"]}' for i in races]
            names += [f'{data_i["é–‹å‚¬åœ°"]} {i}R {data_i[i]["name"]}' for i in races]

    st.session_state["id2name"] = {}
    click3 = st.button("OK")
    if click3:
        st.session_state["id_list"] = []
        for race_id, is_ok, name in zip(id_list, options, names):
            if is_ok:
                st.session_state["id_list"].append(race_id)
                st.session_state["id2name"][race_id] = name
        st.button("æ¬¡ã¸")


elif "df_pred_test_groupby" not in st.session_state:
    seed=42
    past_race = False
    race_id = st.session_state["id_list"]

    df_test_race, df_test_result = get_new_race_csv(race_id)
    df_test_prof, df_test_form = get_new_horse_csv(df_test_result)
    nm = pickle.load(open("nm.sav", 'rb'))

    # # å‰å‡¦ç†
    df_test_race = preprocess_race(df_test_race)
    df_test_result = preprocess_result(df_test_result)
    df_test_prof = preprocess_prof(df_test_prof)
    df_test_form = preprocess_form(df_test_form)
    df_test_form = preprocess_v2(df_test_form)

    # SHIFT
    n_shift = 3
    df_test_form = df_test_form.reset_index()
    df_test_form_shifted = get_shift_form_v2(df_test_form.copy(), n_shift)
    # CONCAT
    df_test = df_concat(df_test_result, df_test_race, df_test_prof, df_test_form_shifted)
    # LABEL ENCODING
    df_test["å¹´é½¢åˆ¶é™"] = df_test["å¹´é½¢åˆ¶é™"].map(lambda x: f"{int(x[0])}{x[1:]}")
    df_test["ä¹—ã‚Šæ›¿ã‚ã‚Š"] = (df_test["jockey_id"].astype(str) != df_test["jockey_id_1"].astype(str)).value_counts()
    df_test["è·é›¢å·®"] = (df_test["è·é›¢"] - df_test["è·é›¢_1"])
    df_test["è·é›¢æ¯”"] = df_test["è·é›¢"] / df_test["è·é›¢_1"]
    df_test["å¹´é½¢flaot"] = (df_test["æ—¥ä»˜"] - df_test["ç”Ÿå¹´æœˆæ—¥"]).map(lambda x: x.days)/365



    for i in range(1, n_shift+1):
        df_test[f"ã‚ªãƒƒã‚º_log_{i}"] = df_test[f"ã‚ªãƒƒã‚º_{i}"]
    for col in ["ã‚ªãƒƒã‚º", "ã‚ªãƒƒã‚º_log", "äººæ°—", "ç€å·®", "ç€é †", "æ¨™æº–ç€å·®", "æ¨™æº–ã‚¿ã‚¤ãƒ ", "ä¸Šã‚Š", "é€šéå¹³å‡", "race_ä¸Šã‚Šå·®"]:
        df_test[f"{col}_{n_shift}_ave"] = np.nanmean(df_test[[f"{col}_{i}" for i in range(1, n_shift+1)]], axis=1)
        df_test[f"{col}_{n_shift}_ave"] = np.mean(df_test[[f"{col}_{i}" for i in range(1, n_shift+1)]], axis=1)
        
    df_test_encoded = encoded(df_test.copy(), download=True)

    with st.spinner('Load AI models'):
        models_data = pickle.load(open("models_data_v7.sav", 'rb'))

    def test(models, _df, _df_encoded, X_cols, TARGET, dev_reverse):
        print(f"        {TARGET}")

        ## set X
        X_test = _df_encoded[X_cols].values

        ## eval
        pred_test = np.zeros(_df_encoded.shape[0])
        for i, model in enumerate(models):
            if "Regressor" not in str(type(model)):
                pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)[:, 1]
            else:
                pred = model.predict(X_test, num_iteration=model.best_iteration_)
            pred_test += pred
        pred_test /= len(models)


        df_test_pred = _df[["ç€é †", "race_id"]]
        df_test_pred[f"{TARGET}_pred"] = pred_test

        if dev_reverse:
            dev_test = arr_dev(df_test_pred, f"{TARGET}_pred", order="reverse")[0]
        else:
            dev_test = arr_dev(df_test_pred, f"{TARGET}_pred", order="foreard")[0]
        
        return pred_test, dev_test

    is_test = (df_test["race_id"] != df_test["race_id_1"]) & (df_test["race_id"] != df_test["race_id_2"]) & (df_test["race_id"] != df_test["race_id_3"])
    if past_race:
        is_test &= (df_test.index % 2 == 1)


    df_pred_test = df_test[is_test]
    df_pred_test_encoded = df_test_encoded[is_test]

    n = len(models_data)
    xxx = st.text(f"infer {0}/{n}")
    bar = st.progress(0.0)
    i = 0


    for learn_level in [1,2,3]:
        print(f"Level {learn_level}")
        for TARGET in models_data.keys():
            if models_data[TARGET]["level"] == learn_level:
                X_cols = models_data[TARGET]["X_cols"]
                models = models_data[TARGET]["models"]
                dev_reverse = models_data[TARGET]["dev_reverse"]

                pred_test, dev_test = test(models, df_pred_test, df_pred_test_encoded, X_cols, TARGET, dev_reverse)

                df_pred_test[f"{TARGET}_pred"] = pred_test
                df_pred_test[f"{TARGET}_dev"] = dev_test

                df_pred_test_encoded[f"{TARGET}_pred"] = pred_test
                df_pred_test_encoded[f"{TARGET}_dev"] = dev_test

                i += 1
                xxx.text(f"infer {i}/{n}")
                bar.progress(i/n)
                
    st.button("AIäºˆæ¸¬çµæœã‚’è¦‹ã‚‹")
    st.session_state["df_pred_test_groupby"] = df_pred_test.groupby("race_id")
    st.session_state["nm"] = nm


else:
    def display_pred(df_pred_test, display_cols, sort_key):

        df_target = df_pred_test[["æ ç•ª", "é¦¬ç•ª"] + display_cols].fillna(0).astype({"é¦¬ç•ª":int, "æ ç•ª":int})
        df_target.index = df_pred_test["horse_id"].map(lambda x: nm[x]).values
        # df_target["ã‚¹ã‚³ã‚¢"] = df_target[
        #     [f"{col}_dev" for col in ["å‹åˆ©", "é€£å¯¾",  "è¤‡å‹", "ç€å·®", "ã‚¿ã‚¤ãƒ "]]
        #     ].mean(axis=1)
        df_target = df_target.sort_values(by=sort_key, ascending=False)
        df_target = df_target.drop_duplicates()

        df_target.columns = [col.replace("_dev", "").replace("ensemble", "") for col in df_target.columns]
        display_cols = [col.replace("_dev", "").replace("ensemble", "") for col in display_cols]

        d = {}
        for name, (umaban, wakuban) in zip(df_target.index, df_target[["é¦¬ç•ª", "æ ç•ª"]].values):
            d[name] = wakuban
            d[umaban] = wakuban

        def back_color(s):
            return [f'background-color: #333333' for i in s]

        def wakuban_color(s):

            alpha = "4D"
            color = ["", f"#ffffff{alpha}",  f"#000000{alpha}", f"#ff0000{alpha}", f"#0000ff{alpha}", f"#ffff00{alpha}",  f"#008000{alpha}", f"#ffa500{alpha}", f"#ffc0cb{alpha}"]
            return [f'background-color: {color[i]}' for i in s]

        def umaban_color(s=""):
            s = [d[i] for i in list(s)]
            alpha = "2D"
            color = ["", f"#ffffff{alpha}",  f"#000000{alpha}", f"#ff0000{alpha}", f"#0000ff{alpha}", f"#ffff00{alpha}",  f"#008000{alpha}", f"#ffa500{alpha}", f"#ffc0cb{alpha}"]
            return [f'background-color: {color[i]}' for i in s]

        def highlight(s):
            if s>50:
                color = f"#ff1493"
            elif s<=50:
                color = f"#00bfff"
            alpha = hex(int(abs(s-50))*6)[2:]
            if len(alpha) == 1:
                alpha = "0" + alpha

            return f'background-color: {color}{alpha}'

        def max_bold(s):
            is_max = (s == s.max())
            
            return ['font-weight:bold; border:solid;' if v else '' for v in is_max]
        
        used = {}
        is_use = []
        for i, name in enumerate(df_target.index):
            if name in used:
                is_use.append(False)
            else:
                is_use.append(True)
            used[name] = i
        df_target = df_target[is_use]

        race_id = df_pred_test.iloc[0].race_id
        df_target = (
                df_target.style
                    .set_precision(1)
                    .apply(back_color)
                    .apply(wakuban_color, subset=['æ ç•ª'])
                    .apply(umaban_color, subset=['é¦¬ç•ª'])
                    .applymap(highlight, subset=display_cols)
                    .apply(max_bold, subset=display_cols)
                    .set_caption(f"{int(race_id[12:14])}å›{df_pred_test.iloc[0].é–‹å‚¬åœ°}{int(race_id[14:16])}æ—¥ç›® {int(race_id[16:18])}R {str(nm[race_id]).replace('()','')} {df_pred_test.iloc[0].èŠãƒ€} {df_pred_test.iloc[0].è·é›¢:.0f}m")    
            )
        return df_target 

    sort_key = "å‹åˆ©"


    race_list = st.session_state["id2name"].values()
    nm = st.session_state["nm"]
    print(race_list)
    options = st.multiselect(
        'äºˆæ¸¬ã™ã‚‹ãƒ¬ãƒ¼ã‚¹ã‚’é¸ã¶',
        race_list,
        race_list)

    display_cols_opt = ["å‹åˆ©", "é€£å¯¾",  "è¤‡å‹", "ç€å·®", "ã‚¿ã‚¤ãƒ ", "å˜å‹æ‰•æˆ»", "è¤‡å‹æ‰•æˆ»"]
    display_cols_opt = ["å‹åˆ©"]

    # display_cols = st.sidebar.multiselect('è¡¨ç¤ºã™ã‚‹æŒ‡æ¨™', display_cols_opt, display_cols_opt)
    display_cols = display_cols_opt
    # display_cols = [f"{col}ensemble_dev" for col in display_cols]
    display_cols = [f"{col}_dev" for col in display_cols]


    click4 = st.button("æ›´æ–°")

    if click4:
        for race_id, df_pred_testi in st.session_state["df_pred_test_groupby"]:
            race_name = st.session_state["id2name"][re.sub(r"\D", "", race_id)]
            if race_name in options:
                st.write(race_name)
                n = df_pred_testi.shape[0] + 1
                df_display = display_pred(df_pred_testi, display_cols, sort_key)
                st.dataframe(df_display, height=35*n)
